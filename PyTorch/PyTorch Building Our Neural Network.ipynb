{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89f0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10,shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca50d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn ##Object oriented programming, requires initializing things\n",
    "import torch.nn.functional as F #Contains lots of functions, need to pass parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d0fd119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Building the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) \n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        self.fc3 = nn.Linear(64, 64) \n",
    "        self.fc4 = nn.Linear(64, 10) \n",
    "    \n",
    "    def forward(self,x): #The method that defines how the data will flow through the network\n",
    "        x = F.relu(self.fc1(x)) #X passes through fully connected layer 1. F.relu is the activation function. \n",
    "                                #Stands for \"rectified linear\", it's either active or it's not.\n",
    "        x = F.relu(self.fc2(x)) #X is redefined and passed through fully connected layer 2, etc.\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) #Data is just passed to 4, it's not activated. The activation function is run on the output.\n",
    "            #The goal is to show which class is selected with a probability distribution on the output\n",
    "        return F.log_softmax(x, dim=1) #The output is a batch of tensors of probability distributions.\n",
    "                                       #If dim were to =0, we'd be distribution across batches. With dim=1, we're distributing\n",
    "                                       #across the output layer tensors\n",
    "       \n",
    "\n",
    "    \n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "#super --> initialization of nn.module. Defining the fully-connected layers to the neural network\n",
    "#Format: self.fc1 = nn.Linear(input, output)\n",
    "#input = input images from the data-->784=28*28. Need to pass the flattened image\n",
    "#Output = 3 layers of 64 nodes for the hidden layers. This can be whatever we want, but we're going with 64\n",
    "#nn.Linear = fully connected neural network\n",
    "#For self.fc2, the previous layer outputs 64, fc2 needs to take in 64. Output is still whatever we want it to be.\n",
    "#Output layer of fc4 only has 10 outputs because we have 10 classes: digits 0 to 9.\n",
    "\n",
    "\n",
    "\n",
    "#The layers are defined, but they don't have a path to take through the layers. We're going to used Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc31ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.6379e-01, 5.9441e-02, 2.4479e-01, 2.2510e-01, 5.3719e-01, 8.1988e-02,\n",
       "         5.2100e-01, 7.5905e-01, 8.0735e-01, 5.4350e-01, 8.1206e-01, 4.3662e-01,\n",
       "         2.5564e-01, 2.8489e-01, 7.8546e-01, 2.8004e-01, 9.4542e-01, 5.3788e-01,\n",
       "         9.4180e-01, 9.2032e-01, 8.2502e-01, 2.4136e-01, 1.0047e-01, 7.5425e-01,\n",
       "         6.8503e-01, 4.6010e-01, 9.2078e-01, 2.5537e-01],\n",
       "        [6.4456e-01, 3.3165e-01, 1.1791e-01, 3.7207e-01, 4.7573e-01, 9.0808e-01,\n",
       "         2.9885e-01, 3.1651e-01, 8.2023e-01, 1.0947e-01, 8.8690e-01, 2.5897e-01,\n",
       "         1.0487e-01, 1.7253e-01, 4.8910e-01, 7.6729e-01, 1.6653e-01, 8.5624e-01,\n",
       "         7.3375e-01, 4.9507e-01, 2.4630e-01, 4.0087e-01, 4.5676e-01, 5.5024e-01,\n",
       "         9.1131e-01, 9.2202e-01, 9.7953e-01, 6.6076e-01],\n",
       "        [4.6891e-01, 2.8643e-01, 7.8704e-01, 5.6443e-01, 3.7518e-01, 6.3746e-01,\n",
       "         3.3159e-02, 7.2843e-01, 2.5250e-01, 2.0442e-01, 9.3485e-01, 2.3461e-01,\n",
       "         8.0097e-01, 2.3791e-01, 3.4927e-01, 5.0986e-01, 3.0932e-01, 3.3680e-01,\n",
       "         2.7929e-01, 6.1133e-01, 9.4006e-01, 2.8606e-01, 5.0505e-01, 2.7365e-01,\n",
       "         9.1596e-01, 2.5490e-02, 9.7682e-02, 7.9092e-01],\n",
       "        [5.4117e-01, 5.0493e-01, 1.4350e-01, 3.0751e-02, 8.6621e-01, 9.7201e-01,\n",
       "         8.3320e-01, 8.9035e-01, 7.7153e-01, 2.2314e-01, 2.1583e-01, 7.3768e-01,\n",
       "         6.6242e-01, 3.0180e-02, 7.5052e-01, 5.3656e-01, 7.4721e-02, 7.4608e-01,\n",
       "         3.0647e-01, 6.9261e-01, 1.5843e-01, 2.7809e-01, 4.2049e-01, 4.2015e-01,\n",
       "         2.6192e-02, 1.7628e-01, 2.6732e-01, 4.6391e-02],\n",
       "        [5.4911e-01, 4.2897e-01, 9.3970e-02, 7.4854e-01, 9.9016e-01, 8.1568e-01,\n",
       "         7.0185e-01, 8.7223e-01, 9.7018e-01, 7.7393e-01, 9.2617e-01, 3.2446e-01,\n",
       "         4.3727e-01, 9.4939e-01, 2.0859e-01, 9.9340e-01, 9.4706e-01, 7.5682e-01,\n",
       "         9.2869e-01, 5.6344e-01, 3.8429e-01, 8.6025e-01, 6.0122e-01, 6.6514e-01,\n",
       "         9.6498e-01, 4.6359e-01, 9.7551e-01, 5.5226e-02],\n",
       "        [1.1217e-01, 8.7983e-01, 9.3912e-01, 5.3663e-01, 4.4685e-01, 1.6553e-01,\n",
       "         5.4131e-01, 2.8871e-01, 6.9052e-01, 3.9270e-01, 6.7004e-01, 3.1023e-01,\n",
       "         6.2048e-01, 1.8952e-01, 9.2393e-01, 1.7405e-01, 3.5217e-01, 6.8596e-01,\n",
       "         2.7260e-01, 1.3094e-01, 6.8343e-01, 4.2824e-01, 9.7024e-01, 8.8229e-01,\n",
       "         3.2954e-01, 6.6533e-01, 9.5712e-01, 9.5078e-01],\n",
       "        [1.0406e-01, 8.6295e-01, 1.3273e-01, 7.0699e-01, 1.0600e-01, 5.3838e-01,\n",
       "         2.2161e-01, 8.2634e-01, 9.6011e-01, 2.9828e-01, 6.6054e-01, 1.0425e-01,\n",
       "         1.6758e-01, 2.9799e-01, 7.8724e-01, 4.9362e-01, 4.5832e-01, 3.8431e-01,\n",
       "         9.9371e-01, 3.4196e-01, 7.3374e-01, 9.5696e-01, 8.8504e-01, 5.8545e-01,\n",
       "         6.2631e-01, 1.3199e-01, 2.3774e-01, 5.5108e-01],\n",
       "        [6.1876e-01, 4.4112e-01, 8.2312e-01, 3.8533e-01, 1.0119e-01, 1.7124e-04,\n",
       "         1.7720e-01, 4.8600e-01, 4.9538e-01, 5.5449e-01, 2.0972e-02, 9.8942e-01,\n",
       "         4.2800e-01, 6.2134e-01, 9.8912e-01, 1.7044e-01, 2.5576e-01, 4.2417e-01,\n",
       "         5.2979e-02, 6.8080e-01, 1.8220e-01, 5.7370e-01, 6.4746e-01, 4.7404e-01,\n",
       "         4.4058e-02, 4.8135e-01, 8.6956e-01, 5.3092e-01],\n",
       "        [9.1173e-01, 4.7303e-01, 6.8173e-01, 4.1668e-01, 6.9360e-01, 2.1512e-01,\n",
       "         9.8819e-02, 8.0635e-01, 7.9604e-01, 5.2184e-01, 1.3786e-02, 7.1968e-01,\n",
       "         9.0179e-01, 1.5185e-01, 6.7628e-01, 6.3741e-01, 3.0510e-01, 6.0155e-01,\n",
       "         9.5257e-01, 3.8377e-01, 3.9628e-01, 6.7488e-01, 8.0809e-01, 3.4988e-01,\n",
       "         1.0258e-02, 4.7081e-01, 2.4614e-02, 5.7534e-03],\n",
       "        [6.7336e-01, 1.1304e-01, 3.4851e-01, 8.7216e-01, 3.1483e-01, 4.2666e-01,\n",
       "         5.5727e-02, 6.2169e-01, 2.8766e-01, 6.2730e-02, 1.2596e-01, 3.5635e-01,\n",
       "         7.7176e-01, 3.2847e-01, 5.8256e-01, 7.5582e-01, 2.2373e-01, 1.2528e-01,\n",
       "         6.1712e-01, 9.4935e-01, 5.0049e-01, 1.2970e-02, 3.3783e-02, 9.8262e-01,\n",
       "         3.9021e-01, 6.9464e-01, 8.0920e-01, 2.5186e-01],\n",
       "        [4.5608e-01, 3.5806e-01, 8.2148e-02, 1.9899e-01, 9.2377e-01, 8.7336e-01,\n",
       "         8.7999e-01, 4.8876e-01, 6.2548e-01, 4.1272e-01, 9.1544e-01, 1.1877e-01,\n",
       "         5.8430e-02, 3.4680e-01, 6.2550e-01, 5.2342e-01, 6.9910e-01, 9.8935e-01,\n",
       "         4.3429e-02, 2.4499e-01, 5.7978e-01, 5.8936e-01, 4.5952e-01, 3.3546e-01,\n",
       "         8.2423e-01, 6.5723e-01, 4.9724e-01, 6.7809e-01],\n",
       "        [2.4717e-01, 9.9828e-01, 4.8080e-01, 4.5054e-01, 5.5427e-01, 6.9945e-01,\n",
       "         3.9467e-01, 7.0101e-02, 9.2902e-01, 7.1170e-01, 2.5849e-01, 8.6537e-01,\n",
       "         4.0471e-01, 1.9574e-01, 8.6448e-01, 6.4853e-01, 2.6813e-01, 4.2843e-01,\n",
       "         7.7879e-01, 2.0431e-01, 2.1662e-01, 3.1890e-01, 9.2012e-01, 2.2373e-02,\n",
       "         8.6513e-01, 4.0456e-02, 5.8009e-01, 6.0425e-01],\n",
       "        [7.7209e-01, 8.0573e-01, 5.0256e-01, 3.2513e-01, 4.6901e-01, 8.6023e-03,\n",
       "         4.9879e-01, 7.4310e-01, 8.0391e-02, 2.6024e-01, 7.9237e-01, 1.5835e-01,\n",
       "         5.9111e-01, 9.3005e-01, 6.8669e-01, 1.6418e-02, 1.2995e-01, 5.7032e-01,\n",
       "         7.8774e-01, 6.6275e-01, 4.6773e-01, 2.8746e-01, 4.1456e-01, 3.9228e-01,\n",
       "         2.9791e-01, 1.7370e-01, 4.0155e-01, 7.2873e-01],\n",
       "        [1.4125e-01, 8.7714e-01, 5.8946e-01, 6.5511e-01, 3.3938e-01, 3.7619e-01,\n",
       "         5.9889e-02, 7.9961e-01, 7.2327e-01, 5.2007e-01, 7.6236e-01, 9.1907e-01,\n",
       "         9.8738e-02, 6.9548e-01, 1.7284e-01, 9.5592e-01, 2.4887e-01, 8.6502e-01,\n",
       "         3.5221e-01, 1.8761e-01, 1.1710e-01, 4.4363e-01, 2.7353e-01, 4.6335e-01,\n",
       "         6.9361e-02, 2.5038e-01, 5.5032e-01, 5.3971e-01],\n",
       "        [5.5352e-01, 3.8347e-01, 7.3030e-01, 1.6177e-01, 4.9730e-01, 6.6856e-01,\n",
       "         1.6687e-01, 3.9234e-01, 3.3448e-01, 8.8008e-02, 4.7724e-01, 5.2450e-01,\n",
       "         8.2093e-01, 6.9210e-01, 3.3720e-02, 2.6051e-01, 9.4459e-01, 9.4228e-01,\n",
       "         4.0557e-01, 3.3219e-01, 2.0895e-02, 2.6029e-02, 6.2446e-01, 1.3332e-01,\n",
       "         1.2748e-01, 8.0835e-01, 2.0983e-01, 1.7840e-01],\n",
       "        [9.1988e-01, 3.6053e-01, 6.3607e-01, 7.3701e-01, 9.1998e-01, 8.6693e-01,\n",
       "         2.7945e-01, 3.4034e-01, 5.0057e-01, 8.9654e-01, 1.6602e-01, 3.3589e-01,\n",
       "         6.8556e-01, 9.6908e-01, 1.7253e-01, 8.0679e-01, 6.2474e-01, 4.6610e-01,\n",
       "         5.0392e-01, 8.9432e-01, 1.2742e-01, 7.5904e-01, 4.5791e-01, 3.7216e-01,\n",
       "         7.6394e-01, 4.9796e-01, 9.7899e-01, 6.9231e-02],\n",
       "        [3.9200e-01, 7.8538e-01, 4.3483e-02, 3.9569e-01, 8.0343e-03, 3.8449e-02,\n",
       "         1.5764e-01, 5.6191e-01, 1.1338e-01, 6.2732e-01, 8.6241e-01, 5.4743e-01,\n",
       "         4.9377e-01, 3.6203e-01, 2.6827e-01, 6.7924e-01, 1.3238e-01, 1.4240e-01,\n",
       "         3.6883e-01, 6.9102e-01, 3.1903e-02, 2.4943e-01, 8.6909e-01, 2.0116e-01,\n",
       "         8.4249e-01, 8.8678e-01, 6.1875e-01, 4.0329e-01],\n",
       "        [3.2037e-01, 7.5949e-01, 5.0250e-02, 9.2090e-01, 6.5615e-01, 8.6078e-01,\n",
       "         5.1203e-01, 2.7859e-01, 4.6743e-01, 6.7823e-01, 7.4247e-01, 3.6572e-02,\n",
       "         8.0277e-01, 5.4505e-01, 9.5542e-02, 5.9023e-01, 4.7924e-01, 8.7529e-01,\n",
       "         7.2167e-01, 3.6939e-01, 2.6591e-01, 3.7669e-01, 4.9670e-01, 8.2758e-01,\n",
       "         5.4406e-01, 5.1290e-01, 7.8023e-01, 2.7737e-01],\n",
       "        [3.0307e-01, 1.5000e-01, 8.1926e-01, 6.8670e-01, 8.3660e-02, 8.5946e-01,\n",
       "         5.0517e-01, 7.5569e-02, 3.4337e-01, 7.0908e-01, 2.8484e-01, 3.4061e-01,\n",
       "         5.1205e-01, 2.2984e-01, 3.5511e-01, 9.6491e-01, 1.0243e-01, 3.7785e-01,\n",
       "         5.9640e-01, 1.8896e-01, 8.1390e-01, 6.0510e-01, 3.5953e-01, 3.7694e-01,\n",
       "         2.1509e-01, 8.8238e-01, 8.1478e-01, 7.5194e-01],\n",
       "        [5.4553e-01, 3.0043e-01, 6.3351e-01, 7.9883e-02, 1.6582e-01, 5.5162e-01,\n",
       "         1.1122e-01, 9.7319e-01, 1.0249e-01, 8.7729e-01, 1.5392e-01, 4.7049e-01,\n",
       "         9.6117e-01, 8.9913e-01, 2.9753e-01, 9.9076e-01, 2.0699e-01, 3.1161e-01,\n",
       "         7.5339e-01, 8.7051e-01, 8.6442e-01, 3.1843e-01, 6.9910e-01, 1.9080e-01,\n",
       "         6.0836e-01, 9.1903e-01, 4.8065e-01, 5.7063e-01],\n",
       "        [7.8278e-01, 1.4951e-01, 2.1825e-01, 5.1475e-01, 4.7441e-01, 7.8508e-01,\n",
       "         3.8644e-01, 5.7724e-01, 5.1929e-01, 2.6525e-01, 4.2091e-01, 6.2838e-01,\n",
       "         2.1218e-01, 4.2504e-01, 6.3687e-01, 8.4994e-02, 4.7393e-01, 6.7685e-01,\n",
       "         3.4601e-01, 9.3481e-01, 3.9196e-03, 6.3827e-01, 7.7641e-01, 5.3815e-01,\n",
       "         4.9447e-01, 7.3924e-01, 8.0996e-01, 7.5710e-01],\n",
       "        [6.6153e-01, 7.0724e-01, 3.8216e-01, 6.9001e-01, 1.2191e-01, 1.2466e-01,\n",
       "         1.2075e-01, 3.3052e-01, 4.5059e-01, 8.4135e-02, 7.7564e-01, 1.5353e-02,\n",
       "         2.2218e-01, 4.9336e-01, 8.3925e-01, 8.7357e-02, 1.2648e-01, 7.3729e-01,\n",
       "         7.4507e-01, 4.9986e-01, 9.4984e-01, 3.6637e-01, 6.5155e-01, 9.1381e-01,\n",
       "         1.7126e-01, 8.3615e-02, 3.6205e-01, 6.0422e-01],\n",
       "        [8.2848e-01, 1.1969e-02, 9.0160e-01, 7.8347e-01, 1.4612e-01, 3.9728e-01,\n",
       "         2.6651e-01, 3.4366e-01, 5.4056e-01, 4.1244e-02, 7.6115e-01, 5.7911e-01,\n",
       "         1.7491e-01, 2.5377e-02, 9.4734e-01, 9.7102e-01, 4.2237e-01, 7.2402e-01,\n",
       "         7.4807e-01, 1.3160e-01, 8.3650e-02, 8.9258e-01, 6.5043e-01, 7.1288e-01,\n",
       "         7.6255e-01, 2.8303e-01, 6.9703e-01, 9.2924e-01],\n",
       "        [7.7562e-01, 6.0888e-01, 5.4077e-02, 1.4884e-01, 2.8270e-01, 3.5851e-01,\n",
       "         8.1732e-01, 3.0885e-01, 5.0661e-01, 1.7797e-01, 9.0599e-01, 7.6547e-01,\n",
       "         1.8958e-01, 4.1485e-04, 9.1505e-02, 1.8492e-01, 3.4177e-04, 9.3808e-01,\n",
       "         4.8088e-02, 8.8987e-01, 3.4137e-01, 1.7335e-01, 3.3718e-01, 4.3669e-01,\n",
       "         4.9451e-01, 4.3317e-01, 5.9541e-01, 7.4877e-01],\n",
       "        [8.9720e-03, 9.5342e-01, 5.5108e-01, 1.6642e-01, 3.8832e-01, 6.2952e-01,\n",
       "         4.2763e-01, 5.9960e-01, 9.7745e-01, 4.3872e-01, 7.2712e-02, 8.7081e-01,\n",
       "         5.0242e-01, 2.7311e-01, 3.0490e-02, 8.3250e-01, 4.6299e-01, 9.9722e-01,\n",
       "         4.1543e-01, 7.7015e-01, 3.3519e-02, 4.3071e-02, 5.1570e-01, 1.8925e-01,\n",
       "         2.9435e-01, 5.3024e-01, 9.8940e-01, 4.1955e-01],\n",
       "        [2.2566e-02, 7.3672e-01, 2.5252e-01, 8.2877e-01, 8.3070e-01, 5.1099e-01,\n",
       "         4.7734e-01, 8.3156e-01, 8.1361e-01, 2.8371e-01, 6.1264e-01, 7.6901e-01,\n",
       "         8.9214e-01, 3.0672e-01, 7.3085e-01, 2.1258e-01, 1.5197e-02, 7.2342e-01,\n",
       "         3.0653e-01, 8.6511e-01, 6.9725e-01, 7.2888e-01, 7.7400e-01, 4.1928e-01,\n",
       "         3.8425e-01, 7.6865e-01, 6.2283e-01, 8.5635e-01],\n",
       "        [4.7094e-02, 2.2285e-01, 5.8043e-01, 4.0463e-01, 3.5315e-01, 8.7865e-01,\n",
       "         6.2976e-01, 6.8727e-01, 8.7734e-02, 8.1637e-01, 5.4913e-01, 9.1779e-01,\n",
       "         7.0905e-01, 9.5154e-01, 4.2077e-01, 7.3739e-01, 8.9408e-01, 1.7487e-01,\n",
       "         2.4042e-01, 6.6285e-01, 3.6828e-01, 4.6579e-02, 8.8855e-02, 7.3223e-01,\n",
       "         8.9297e-01, 7.5889e-01, 1.3900e-01, 8.8146e-01],\n",
       "        [4.5461e-01, 3.6230e-01, 9.2090e-01, 3.3177e-01, 5.0404e-01, 9.7130e-02,\n",
       "         8.3486e-01, 9.0651e-01, 6.2392e-01, 7.5165e-01, 3.0598e-01, 3.2743e-03,\n",
       "         2.1352e-01, 2.0225e-01, 4.2640e-01, 4.7177e-01, 4.6667e-01, 6.0588e-01,\n",
       "         7.9290e-01, 2.8303e-01, 4.6833e-01, 8.5079e-01, 4.1281e-01, 2.1220e-01,\n",
       "         9.8761e-02, 2.6227e-01, 6.1695e-01, 6.6176e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing data through the neural network\n",
    "\n",
    "X = torch.rand((28,28))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42fcdc5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-24c90224343f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#What happens if we try to pass the X data as-is?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-d60220e0c68c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#The method that defines how the data will flow through the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#X passes through fully connected layer 1. F.relu is the activation function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                                 \u001b[1;31m#Stands for \"rectified linear\", it's either active or it's not.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#X is redefined and passed through fully connected layer 2, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)"
     ]
    }
   ],
   "source": [
    "#What happens if we try to pass the X data as-is?\n",
    "output = net(X)\n",
    "\n",
    "##Ewwie errors. Size mismatch. Needs to be FLATTENED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed32223f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b2affa4a8b31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-d60220e0c68c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Data is just passed to 4, it's not activated. The activation function is run on the output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m#The goal is to show which class is selected with a probability distribution on the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#The output is a batch of tensors of probability distributions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                                        \u001b[1;31m#If dim were to =0, we'd be distribution across batches. With dim=1, we're distributing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                        \u001b[1;31m#across the output layer tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "#Let's try it again by making it a 28*28\n",
    "\n",
    "X = X.view(28*28)\n",
    "output = net(X)\n",
    "\n",
    "#Still some nasty errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "845184ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3407, -2.5028, -2.3920, -2.2320, -2.2502, -2.2687, -2.2515, -2.2567,\n",
       "         -2.2576, -2.3048]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The data needs to be formatted exactly how the libraries want them. \n",
    "\n",
    "X = X.view(-1,28*28) #Recall: the -1 specifies this will be of an unknown shape. -1 says it will be a tensor of any size\n",
    "output = net(X)\n",
    "output\n",
    "#The output is the actual predictions for the digits 0 to 9\n",
    "#Looks nasty, but weights haven't been initialized and it hasn't been trained, but data was successfully passed through\n",
    "#The next step is to see how far off we were from the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac9ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
